{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Business Problem: \n",
    "\n",
    "To classify a car as **acceptable** , **unacceptable** , **good** or **very good** based on its price , characterstics and maintenance cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Description : \n",
    "\n",
    " - **Model concept structure**:\n",
    "\n",
    "      \n",
    "       Price                    overall price\n",
    "       Maintenance Cost         price of the maintenance\n",
    "       Number of Doors          number of doors\n",
    "       Capacity                 capacity in terms of persons to carry\n",
    "       Size of Luggage boot     the size of luggage boot\n",
    "       safety                   estimated safety of the car\n",
    "\n",
    "  \n",
    " - **Number of Instances**: 1728\n",
    "   (instances completely cover the attribute space)\n",
    "\n",
    " - **Number of Attributes**: 6\n",
    "\n",
    " - **Attribute Values**:\n",
    "\n",
    "       Price                   v-high, high, med, low\n",
    "       Maintenance Cost        v-high, high, med, low\n",
    "       Number of Doors         2, 3, 4, 5-more\n",
    "       Capacity                2, 4, more\n",
    "       Size of Luggage boot    small, med, big\n",
    "       safety                  low, med, high\n",
    "\n",
    " - **Missing Attribute Values**: None\n",
    "\n",
    " - **Class Distribution (number of instances per class)**\n",
    "\n",
    "       class      N          N[%]\n",
    "       -----------------------------\n",
    "       unacc     1210     (70.023 %) \n",
    "       acc        384     (22.222 %) \n",
    "       good        69     ( 3.993 %) \n",
    "       v-good      65     ( 3.762 %) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Approach : \n",
    "\n",
    "You will understand the dataset and build a model in the following stages: \n",
    "\n",
    "- **Data Specifications and Cleaning**\n",
    "- **Exploratory Data Analysis**\n",
    "      - Uni-Variate Analysis : Pie Charts\n",
    "      - Bi-Variate Analysis : Stacked Bar Graphs , Viloin Plots and Box Plots\n",
    "- **Data Processing , Label Encoding**\n",
    "- **Data Splitting into Train and Test sets**\n",
    "- **Modelling and Hypertuning**\n",
    "      - KNN Classifier, HyperTuning\n",
    "      - Random Forest, HyperTuning: Grid Search\n",
    "- **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1` Data Specifications and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwBONYC8NeRD"
   },
   "source": [
    "**`1.1` Importing basic Libraries and Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3rIkXy8wJQg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "VFpfrCsmzV2J",
    "outputId": "5991a102-89a8-42e9-f78f-8aa6c4822039"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/kusht/Downloads/car_evaluation.csv',header=None)\n",
    "# Assigning names to the columns in the dataset\n",
    "\n",
    "data.columns = ['Price', 'Maintenance Cost', 'Number of Doors', 'Capacity', 'Size of Luggage Boot', 'safety', 'Decision']\n",
    "\n",
    "# Original dataset doesnt have name so putting them manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDMyyYFlbuL2"
   },
   "source": [
    "**TASK : Print the shape of the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (~ 1 line of code)\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2Zy-kTiN1Qi"
   },
   "source": [
    "**`1.2` Data Insights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "OGX946OsYTjU",
    "outputId": "8bc5b307-20dc-4718-895a-ef39a40b98a8"
   },
   "source": [
    "**TASK : Print information about dataset using `info()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "97PuHNN6YYjm",
    "outputId": "c884b6e2-62ef-4209-a67f-0266dccb660d"
   },
   "outputs": [],
   "source": [
    "###ENTER CODE HERE (~ 1 line of code)\n",
    "\n",
    "###END CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK: Describe the dataset using `describe` function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "1Zj_vix3dCM3",
    "outputId": "12019106-6dfa-4308-960c-5018c721c649"
   },
   "outputs": [],
   "source": [
    "###ENTER CODE HERE (~ 1 line of code)\n",
    "\n",
    "###END CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK: Print counts of each value of each attribute using for loop and `value_counts()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "PIrM7DT7Ye8e",
    "outputId": "5e2c7ac5-81b9-439d-d061-a391b2fab63d"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE (~1 Line of code)\n",
    "for i in data.columns : \n",
    "    # write code to print value_counts \n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "SaLWF-c5YiCQ",
    "outputId": "f0ab5b7e-8513-4562-ac5c-f8c16a5d8424"
   },
   "source": [
    "Seeing the basic characterstics of the dataset you can now analyse whether the dataset is balanced , skewed , shape and make a model keep the following details in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2` Exploratory Data Analysis \n",
    "\n",
    "After getting insight about the data , now you'll understand the data better by visualisations by doing Uni-Variate and Bi-Variate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iU_CFnjJNSsr"
   },
   "source": [
    "## `2.1` Uni-Variate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - PIE CHARTS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Make `Pie chart` for `Decision`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINT** : Figure out the sizes of each pie wedge by value_count of that instance . Use `value_counts()` to find sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "colab_type": "code",
    "id": "9sLB7sPdeVac",
    "outputId": "1f34b78a-e516-4a08-bd15-5ba80089dbc0"
   },
   "outputs": [],
   "source": [
    "## Make a PIE chart of `decision`\n",
    "\n",
    "## START CODE HERE : (FULL CODE , FILL THE PLACES WITH A SINGLE HASH)\n",
    "\n",
    "# labels are names of elements in the feature that we want to ouput in the pie chart \n",
    "labels = #\n",
    "\n",
    "# Colors provides the colors of the pie wedges\n",
    "colors = #\n",
    "\n",
    "# Size of pie wedges are given by using values from value_counts method which we acquired \n",
    "size = #\n",
    "\n",
    "# Explode provides the spacing between each pie wedge \n",
    "explode = #\n",
    "\n",
    "# Set the figure size to (6,6)\n",
    "\n",
    "# Plot Pie chart \n",
    "### autopct shows the percentage on each wedge \n",
    "\n",
    "# Use plt.pie and use the above set variables as arguments to create a pie chart \n",
    "\n",
    "# Set Title , Axis = 'off' and Legend\n",
    "    \n",
    "# Show the plot \n",
    "\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse this graph and justify the imbalance with a suitable reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : MAKE A `PIE CHART` OF `Price`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - print the value counts of each element in attribute `Price` using `value_counts` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "jU4YE4kzdgde",
    "outputId": "a13dcb0d-a9d8-4223-a5eb-f49120fe9069"
   },
   "outputs": [],
   "source": [
    "###ENTER CODE HERE (~1 Line of code)\n",
    "\n",
    "###END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make similar <i>pie chart</i>  for `Price` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "colab_type": "code",
    "id": "hq1fa9jwf3Hn",
    "outputId": "d3db126d-324a-4b6b-8a72-b24c9936d672"
   },
   "outputs": [],
   "source": [
    "## PIE CHART OF 'PRICE'\n",
    "\n",
    "### START CODE HERE : (FULL CODE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GOOD WORK! \n",
    "However , its clearly visible that since all other attributes are also equally balanced , all the univariate visualisation would be same and of equally balanced classes , so percenatges would always be `33.3%` for three elements and `25%` for four elements in an attribute this completes Uni-Variate Analaysis and lets move on to Bi-Variate analysis to understand data better! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.2` Bi-Variate Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - STACKED BAR GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STACKED BAR GRAPH** is a very convenient and easy to understand visualisation for two categorical variables and to make them we'll use a important method from pandas , `crosstab` . The role of crosstab is to form a separate cross table between the given attributes where the values are frequencies/counts having the specific features of the attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Understand `crosstab` function by making one between `Price` and `Decision`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of crosstab between price and decision : \n",
    "# Put the variables you want the crosstab between in place of '#'\n",
    "pd.crosstab(# , #)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK  : Make `Stacked Bar Graph` between all attributes and `Decision` using for loop and crosstab function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making STACKED BAR graphs between all attributes and 'decision'\n",
    "\n",
    "for i in data.columns : \n",
    "    ## Write the variables for crosstab. \n",
    "    ## Remember one variable 'decision' is fixed and other variable 'i' gives the column names\n",
    "    ctab = pd.crosstab(#,#)\n",
    "    \n",
    "    ## Dividing by sum to give range between 0 and 1\n",
    "    ## Write the arguments of plot() in place of '#' . To create bar plot use kind='bar' and dont forget to keep stacked='True'\n",
    "    ## Keep the size of bar as (6,6)\n",
    "    ctab.div(ctab.sum(1).astype(float), axis = 0).plot(#)\n",
    "    \n",
    "    ## Write Title , labels and legend\n",
    "    ## Use '{}.format()' function to write title corresponding to that each particular column\n",
    " \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do the above exercise manually writing individual plots for each attribute but for loop makes it very convenient and effortless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GOOD WORK! \n",
    "Now you have plotted stacked bar graphs of all attributes . Analyse it and gain insights about the data . Ahead you'll use another visualise technique called `VIOLIN PLOTS` but that visualisation is only for numerical attributes . So first use `labelencoder` or `hard code` by manually giving values to convert them to numerical attribute and then plot the violin plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rBsjX_xNKzU"
   },
   "source": [
    "## `3` Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3.1` Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all algorithms and many visualisations require numerical data. You have to convert all the categorical attributes into numerical ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `LabelEncoder()` directly or you can Hard Code them by individually giving values to each instance for each attribute. \n",
    "Hard coding is preferred as the values to be given and in which order are exactly known and the attributes and unique elements are less. LabelEncoder would encode the categorical data randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK  : Convert all the categorical attributes to numerical ones using  `LabelEncoder` or Manually `Hard Code` all the categorical attributes as per your choice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE (FULL CODE)\n",
    "\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GREAT! \n",
    "You're done with encoding , now check whether encoding has worked properly or not by seeing the dataset and by using `info()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen there are still attributes of `object` datatype which need to be converted to `int` datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK  : Convert all attributes to `int`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE: \n",
    "\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Violin Plots and Box plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Violin plots` and `Box plots` are a great way to visualise numerical data. The plots conveniently gives us information such as : \n",
    "       \n",
    "   - Median (a white dot on the violin plot and center line in box plot)\n",
    "   - Interquartile range (the black bar in the center of violin and the boundaries of the box in box plot)\n",
    "   - The lower/upper adjacent values (the black lines stretched from the bar) — defined as `first quartile — 1.5 IQR` and `third quartile + 1.5 IQR` respectively. These values can be used in a simple outlier detection technique (Tukey’s fences) — observations lying outside of these “fences” can be considered outliers.\n",
    "   - The stretch in violin plots gives us the relative counts/frequencies of elements having that value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1040/1*TTMOaNG1o4PgQd-e8LurMg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Make  `Violin Plots` between all attributes and `Decision`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have to make violin plots of all attributes . Instead of manually writing the code everytime ,  use `for loop` again to ease out the process . Fill in the spaces where a single `#` is given "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = 'dark', palette = 'colorblind', color_codes = True)\n",
    "### START CODE HERE : \n",
    "\n",
    "## SET THE FIGURE SIZE USING PLT.RCPARAMS :\n",
    "#\n",
    "\n",
    "## SET 5 COLORS OF YOUR CHOICE \n",
    "color = #\n",
    "\n",
    "## PLOT VILOIN PLOTS FOR ALL ATTRIBUTES EXCEPT 'SAFETY'\n",
    "## CREATE A VARIABLE 'COLS' HAVING ALL COLUMN NAMES EXCEPT 'SAFTEY'\n",
    "\n",
    "#\n",
    "\n",
    "## FOR LOOP : \n",
    "for c,i in zip(color,cols) : \n",
    "    ## WRITE CODE FOR VIOLIN PLOT IN THE VARIABLE 'AX' \n",
    "    ## REMEMBER TO PUT 'COLOR' ARGUMENT = c \n",
    "    #\n",
    "    ax.set_title('Violin Plot to show relation between {} and Decision'.format(i), fontsize = 20)\n",
    "    ax.set_xlabel('{} in Increasing range'.format(i), fontsize = 15)\n",
    "    ## SET THE YLABEL TO 'Decision'\n",
    "    #\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : PLOT `BOXPLOT`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot boxplot in the similar way using almost the same code as for previous viloin plots but instead of `sns.violinplot` , use `sns.boxplot` . Create a boxplot betweeen **Safety-Decision** : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "colab_type": "code",
    "id": "4Ff23p_NCegJ",
    "outputId": "ccc7b8f6-396e-4233-a0b2-ff45c28d3b11"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE (FULL CODE)\n",
    "\n",
    "#SET STYLE,FIGSIZE,BOXPLOT,TITLE,LABELS\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the Violin plots and box plots to gain better insight about data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3.2` CORREALTION MATRIX\n",
    "\n",
    "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Make a `Correlation Matrix` using `corr()` and make it visually aesthetic using `sns.heatmap`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE  : (~1 Line of code)\n",
    "\n",
    "### END CODE  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Great!! \n",
    "Its clearly visible since there is no proper co relation between any of the attributes except `decision` prompts that there is no use to do `multi variate analysis` so you've successfully completed the different analysis of data and gained all the information . Now its time to make a model based on the information. For that split the data into train and test set and then make a model on it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4_pQ7n_NEeG"
   },
   "source": [
    "## `4`  Data Split into Train and Test Set  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK  : Split the data into dependent and independent variables and print their shapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "8cObnYX8LPn_",
    "outputId": "b25c1d5a-a441-4c98-d509-d095e83d3832"
   },
   "outputs": [],
   "source": [
    "### Splitting the dataset into dependent and independent variables\n",
    "\n",
    "## START CODE HERE : (~ 2 Lines of code)\n",
    "\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK  : Split the data into train and test sets with test size=0.15, random state = 0 and print their shapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "_j5zU5s7LtK_",
    "outputId": "051b2086-25e3-4d12-ac3d-e117a737894a"
   },
   "outputs": [],
   "source": [
    "## Splitting the dataset into train and test sets\n",
    "## Import the required Library \n",
    "\n",
    "### START CODE HERE : \n",
    "\n",
    "\n",
    "### END CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Y5m1qhDNBQw"
   },
   "source": [
    "## `5`  Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youll be creating models based on two algorithms : \n",
    " - **KNN CLassifier**\n",
    " - **Random Forest** \n",
    "\n",
    "followed by which , model would be improved via `HyperTuning` and finally you'll analyse and choose the best model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYL--4WZQQKU"
   },
   "source": [
    "### `5.1` KNN Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems . The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other , \"Birds of a feather flock together\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/Map1NNReducedDataSet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as can be seen from the image , it forms boundaries of different classes and elements are classified based on which class they lie on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Make a `KNN classifier model` and print `train and test accuracy` along with `confusion matrix`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "49iitxpvSImd",
    "outputId": "09c80dfb-be37-4b45-c927-a899bd436365"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "### Make a KNN MODEL : \n",
    "\n",
    "### START CODE HERE : \n",
    "\n",
    "## CRreating a model :\n",
    "\n",
    "model = # Write the code for a base model of KNeighborsClassifier() with n_jobs=-1\n",
    "\n",
    "## Fit : \n",
    "# fit the model on x_train and y_train \n",
    "\n",
    "# Predict the values for x_test using predict() method and put it in a variable 'y_pred'\n",
    "\n",
    "# Print the training and testing accuracy :\n",
    "\n",
    "# printing the confusion Matrix : \n",
    "\n",
    "### END CODE HERE : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does high accuracy mean that this model is a good predictor?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even a high accuracy doesnt necesssarily imply that its a great model especially with `Imbalanced Multiclass Classification` , Accuracy might give misleading results as model might be predicting everything as `unacc` in this case and since majority of the elements actually give the value `unacc` , accuracy is bound to be high but this does not mean the model is good predictor. So you have to analyse it using other measures like `F1 scores` , `Precision` , `Recall`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better understand the definitive formulas for precision , recall and F1 scores are: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2000/1*6NkN_LINs2erxgVJ9rkpUA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/752/1*UJxVqLnbSj42eRhasKeLOA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`True Positives` , `False positives` and `False negatives` are found out using the earlier calculated confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*CPnO_bcdbE8FXTejQiV2dg.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The significance of each parameters are basically , while `recall` expresses the ability to find all relevant instances in a dataset, `precision` expresses the proportion of the data points our model says was relevant actually were relevant. So a high recall score means that we're operating on relevant instances in the dataset and high `precision` means more are the relevant instances which our model predicted to be relevant and thus high precision and recall generally gives high F1 Score which is favorable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Make a `Classification Report` to properly assess the models performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (~1 Line of code)\n",
    "\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN HYPERTUNING \n",
    "Now try to gain more accuracy by hypertuning the parameter `Number of Neighbors`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to find the best hyperparameter by using the following code below. Though its advised not to lose for loops as computation speed is slower however to visualise the scores we're gonna use them. \n",
    "This can also be done with `GridSearch` which we'll explore in the next model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Use for loop to give values to `n_neighbors` from 2 to 30 and calculate average `cross validation score` for each of them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE : \n",
    "\n",
    "## IMPORT LIBRARY FOR CROSS_VAL_SCORE: \n",
    "\n",
    "avg_score=[]\n",
    "for k in range(2,30):\n",
    "    # PUT ARGUMENTS OF n_jobs and n_neighbors\n",
    "    knn=KNeighborsClassifier(#)\n",
    "    ## CALCULATE CROSS_VAL_SCORE with cv=5 and scoring='accuracy' and store it in variable 'score' : \n",
    "    #\n",
    "    avg_score.append(score.mean())\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Plot the average scores of all `k's`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STRART CODE HERE: \n",
    "\n",
    "## Set the figure size to (12,8)\n",
    "plt.figure(#)\n",
    "# Plot the figure using plt.plot() where x values are range(2,30) and y values the average scores :\n",
    "\n",
    "# Keep the xlabel as n_neighbours and ylabel as accuracy\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the curve and extract the `Top 2 highest accuracies` values for n_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Calculate `Accuracy` and  `F1 score` for both the values of n_neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE : (FULL CODE)\n",
    "\n",
    "# MAKE MODELS FOR both values of n_neighbors , FIT THEM , PREDICT , PRINT ACCURACY AND CLASSIFICATION REPORT\n",
    "\n",
    "### END CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse both the models and choose the best one and write their characterstics here in the blank spaces : \n",
    "\n",
    " - **Optimised KNN model : n_neighbors = _**\n",
    " - **Accuracy ~ `_`**\n",
    " - **F1 Score ~ `_`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will further clear your understanding that better accuracy doesnt necessarily mean better F1 score or better model. One has to analyse everything before finalising a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9SddwhoA6DZ"
   },
   "source": [
    "### `5.2` Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "VWbCyEpPAVg9",
    "outputId": "6f51afc1-a5ef-43e6-e73d-5e0f81f9e740"
   },
   "source": [
    "To understand Random forest classifier , lets first get a brief idea about `Decision Trees` in general. \n",
    "Decision Trees are very intuitive and at everyone have used this knowingly or unknowingly at some point . Basically the model keeps sorting them into categories forming a large tree by responses of some questons (decisions) and thats why its called decision tree. An image example would help understand it better : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1000/1*LMoJmXCsQlciGTEyoSN39g.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "ob_OXoDkC7Oo",
    "outputId": "fc8bd5b1-c2e3-4acd-f535-415e3d65a15a"
   },
   "source": [
    "`Random Forest` : Random forest, like its name implies, consists of a large number of individual decision trees that operate as an [ensemble](https://en.wikipedia.org/wiki/Ensemble_learning) . Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1000/1*VHDtVaDPNepRglIAv72BFg.jpeg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental concept is  large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models. Since this dataset has very low correlation between attributes , random forest can be a good option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Make a Random forest Classifier and print the `Accuracy` and `F1 score`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "vRTsWYdzBN4k",
    "outputId": "15c157ff-7b02-4c02-e2e4-5baf8fa4d818"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "# This allows to use f1_score() function directly\n",
    "\n",
    "### START YOUR CODE HERE : (FULL CODE)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "VOU2_35XDE2p",
    "outputId": "f19e8191-4222-4354-9dd3-ef5b09bc6352"
   },
   "source": [
    "The accuracy and F1 score is the base model measures and now you will hypertune it using `GridSearch` to make it a better model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "1j0UC0uPGV3d",
    "outputId": "da1b25c4-90f7-4ec7-b2c0-2f6b7e76e66e"
   },
   "source": [
    "**Random Forest HyperTuning : Grid Search** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid-search is used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions. It builds a model for every combination of hyperparameters specified and evaluates each model. A more efficient technique for hyperparameter tuning is the Randomized search — where random combinations of the hyperparameters are used to find the best solution. However , if its a small sample like the current dataset then gridsearch is also fine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "-b9-8Y1QGawC",
    "outputId": "64ab7cd7-c838-4044-c0f4-16dc4891ab00"
   },
   "source": [
    "**TASK : Do a `GridSearch` and print the best hyperparameters for this random forest classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the required library : \n",
    "\n",
    "### START YOUR CODE HERE (FULL CODE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK  : Print the `Accuracy` , `Precision` , `Recall` and `F1 Score` for the optimised Random forest model and compare with previous models** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE : (FULL CODE)\n",
    "\n",
    "\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWESOME! \n",
    "Now , you are completely done with `Modelling` and `Hypertuning the Parameters` . The last thing thats left is to write a conclusion stating which model is the best and its different scores and that finishes this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `6` Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the conclusion here"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Car Evaluation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
